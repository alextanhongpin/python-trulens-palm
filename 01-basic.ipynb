{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afd6262-3741-42fb-a246-5bfa20e60cd7",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "https://www.trulens.org/trulens_eval/quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c9927b-0d8a-4d8a-bbc8-10a7447df7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens_eval.feedback.provider.litellm import LiteLLM\n",
    "# litellm_provider = LiteLLM(model_engine='palm', endpoint='localhost:8000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de357e-43d5-4528-aee3-d2ac7212071e",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32047f8f-2f6c-4f49-a41f-38ea0efff869",
   "metadata": {},
   "outputs": [],
   "source": [
    "university_info = \"\"\"\n",
    "The University of Washington, founded in 1861 in Seattle, is a public research university\n",
    "with over 45,000 students across three campuses in Seattle, Tacoma, and Bothell.\n",
    "As the flagship institution of the six public universities in Washington state,\n",
    "UW encompasses over 500 buildings and 20 million square feet of space,\n",
    "including one of the largest library systems in the world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d212af-bc1c-4097-b0e2-6a5513b2ce76",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "\n",
    "Create a chromadb vector store in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef1a1a18-5ea1-460d-8ce4-fb2b2fe50bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alextanhongpin/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████████████████████████| 79.3M/79.3M [00:32<00:00, 2.54MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "students_embeddings = default_ef([university_info])\n",
    "\n",
    "client = chromadb.Client()\n",
    "vector_store = client.create_collection(name=\"Students\")\n",
    "\n",
    "vector_store.add(\n",
    "    embeddings = students_embeddings,\n",
    "    documents = [university_info],\n",
    "    metadatas = [{'source':'university info'}],\n",
    "    ids = [\"id1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5deb2a9-ea8a-452b-8c30-473ddfe594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecf0adef-054e-4ca8-b4f8-1bc4312bf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# https://github.com/BerriAI/litellm?tab=readme-ov-file#step-2-replace-openai-base\n",
    "# set proxy to base_url request sent to model set on litellm proxy, `litellm --model`\n",
    "oai_client = OpenAI(api_key='anything', base_url='http://0.0.0.0:8000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb80727-496c-4b06-9574-443745f3b5ad",
   "metadata": {},
   "source": [
    "## Build RAG from scratch\n",
    "Build a custom RAG from scratch, and add TruLens custom instrumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2f826e3-324c-493c-93ef-06314618116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_from_scratch:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        results = vector_store.query(\n",
    "        query_texts=query,\n",
    "        n_results=2\n",
    "    )\n",
    "        return results['documents'][0]\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        completion = oai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=\n",
    "        [\n",
    "            {\"role\": \"user\",\n",
    "            \"content\": \n",
    "            f\"We have provided context information below. \\n\"\n",
    "            f\"---------------------\\n\"\n",
    "            f\"{context_str}\"\n",
    "            f\"\\n---------------------\\n\"\n",
    "            f\"Given this information, please answer the question: {query}\"\n",
    "            }\n",
    "        ]\n",
    "        ).choices[0].message.content\n",
    "        return completion\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, context_str)\n",
    "        return completion\n",
    "\n",
    "rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06c4bd-f381-49da-93e8-c9494a7cbb07",
   "metadata": {},
   "source": [
    "## Set up feedback functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "587707f2-64b2-4d84-a6fb-441220503331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retrieve.rets.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Context Relevance, input statement will be set to __record__.app.retrieve.rets.collect() .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Select\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider import OpenAI as fOpenAI\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "fopenai = fOpenAI(client=oai_client)\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, \n",
    "             name='Groundedness')\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(fopenai.relevance_with_cot_reasons, name='Answer Relevance')\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fopenai.qs_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "536deb20-1e8a-4ec4-beab-478e3c54e6f4",
   "metadata": {},
   "source": [
    "## Construct the app\n",
    "\n",
    "Wrap the custom RAG with TruCustomApp, add list of feedbacks for eval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "034189bc-6c98-4d3b-92d6-91f6575e57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag = TruCustomApp(rag, \n",
    "                       app_id = 'RAG v1',\n",
    "                       feedbacks = [f_groundedness, f_qa_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dcacf1c-aaa0-469c-987f-154e6e777389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/trulens_eval/feedback/feedback.py\", line 481, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 377, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 303, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 480, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/trulens_eval/feedback/feedback.py\", line 482, in <lambda>\n",
      "    lambda: self.imp(**ins)\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/trulens_eval/feedback/groundedness.py\", line 155, in groundedness_measure_with_cot_reasons\n",
      "    for line in reason.split('\\n'):\n",
      "                ^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'reason' where it is not associated with a value\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/trulens_eval/feedback/feedback.py\", line 486, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of groundedness_measure_with_cot_reasons failed on inputs: \n",
      "{'source': [['\\n'\n",
      "             'The University of Washington, founded in 1861 in Seattle, is a '\n",
      "             'public research u\n",
      "cannot access local variable 'reason' where it is not associated with a value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    rag.query(\"When was the University of Washington founded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19c08a82-94cd-4870-a85b-e84206d009a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dbc9610c014223938326991dd1f2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.8.16:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26a6398e-0f52-4b90-9039-45c920f67f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    rag.query(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "482a2c72-b2b7-412c-83f0-55a067ae57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.stop_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02cd49fcddf742a8ad106326b6bed798": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_ca90371bb10246fbbe1f720f6c828a46",
       "style": "IPY_MODEL_25c0cba84f424c8385f826b875691564",
       "value": "STDOUT"
      }
     },
     "08687710b1b34a5d9f5df8f4fa5d337d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "0fde20c8c07f48d98b4b434dc0bd84f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "25c0cba84f424c8385f826b875691564": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "27fb1ad19ed74bbf871ce4a12a422ef5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2a9922fd7e1048368156a0233947929c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3014a25264e94c9c8e25d5c45228567a",
        "IPY_MODEL_74e6e77b07104f34b028359af083a328"
       ],
       "layout": "IPY_MODEL_7cb3b901ee2747358e6c2baaad6d87db"
      }
     },
     "3014a25264e94c9c8e25d5c45228567a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_7ba2a6b9029f4f49b7e3f37570b97534",
       "style": "IPY_MODEL_08687710b1b34a5d9f5df8f4fa5d337d",
       "value": "STDERR"
      }
     },
     "4a75475260c946afa72bcfc3c36449eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_02cd49fcddf742a8ad106326b6bed798",
        "IPY_MODEL_9a47a3b863e6463b819cde56a3cd7b2f"
       ],
       "layout": "IPY_MODEL_27fb1ad19ed74bbf871ce4a12a422ef5"
      }
     },
     "74e6e77b07104f34b028359af083a328": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_d788eb4f2cf64d349f384eaca79c8d71",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "/Users/alextanhongpin/Documents/python/python-trulens-palm/.venv/lib/python3.11/site-packages/langchain/__init__.py:34: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain.prompts.PromptTemplate instead.\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "  warnings.warn(\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": ""
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "Dashboard closed."
        }
       ]
      }
     },
     "7ba2a6b9029f4f49b7e3f37570b97534": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7cb3b901ee2747358e6c2baaad6d87db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "98a77fab023241f0b70e850ddb332d88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a47a3b863e6463b819cde56a3cd7b2f": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c021b7c4acdd43169402bb12e9c61643",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "  You can now view your Streamlit app in your browser.\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "  Network URL: http://192.168.8.16:8501\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "  External URL: http://103.6.151.184:8501\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "  For better performance, install the Watchdog module:\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "  $ xcode-select --install\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "  $ pip install watchdog\n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "            \n"
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": ""
        },
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "Dashboard closed."
        }
       ]
      }
     },
     "c021b7c4acdd43169402bb12e9c61643": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca90371bb10246fbbe1f720f6c828a46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d1dbc9610c014223938326991dd1f2fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_e464ce212d244f2c8a5bdedd4bba37f4"
       ],
       "layout": "IPY_MODEL_98a77fab023241f0b70e850ddb332d88",
       "selected_index": 0,
       "titles": [
        "Dashboard log"
       ]
      }
     },
     "d788eb4f2cf64d349f384eaca79c8d71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e464ce212d244f2c8a5bdedd4bba37f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4a75475260c946afa72bcfc3c36449eb",
        "IPY_MODEL_2a9922fd7e1048368156a0233947929c"
       ],
       "layout": "IPY_MODEL_0fde20c8c07f48d98b4b434dc0bd84f5"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
